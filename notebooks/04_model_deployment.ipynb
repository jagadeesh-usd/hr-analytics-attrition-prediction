{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Attrition Model: A Production Plan (Managed Stack)\n",
    "\n",
    "This document outlines a complete, production-grade deployment plan for the XGBoost attrition model. The objective is to turn the model into a reliable, scalable, and secure business tool that delivers real-time insights using the fully managed AWS SageMaker stack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deployment Strategy & Objectives\n",
    "\n",
    "We will deploy the model as a **real-time REST API**. This approach supports on-demand use cases, such as integrating predictions directly into an HR portal to provide managers with immediate employee attrition risk scores.\n",
    "\n",
    "**Key Performance Indicators (KPIs) & Targets:**\n",
    "* **Latency Target**: p95 latency of ≤ 200ms for end-to-end predictions.\n",
    "* **Traffic Estimate**: Approximately 500 prediction requests per day.\n",
    "* **Cost Envelope**: Target of ≈ $80/month, covering compute, monitoring, and storage.\n",
    "* **Update Cadence**: Features will be refreshed daily, with a full model retrain scheduled monthly.\n",
    "* **Fallback Plan**: If the real-time API is unavailable for more than 5 minutes, the system will fall back to serving nightly cached batch scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Technology Stack & Architecture (AWS SageMaker)\n",
    "\n",
    "The architecture will be built entirely on the AWS SageMaker platform, leveraging its managed services to create a secure, scalable, and automated MLOps workflow.\n",
    "\n",
    "| Component | AWS SageMaker Service | Purpose |\n",
    "|---|---|---|\n",
    "| **Model Hosting** | **SageMaker Real-Time Endpoint** | Deploys the trained model as a fully managed, auto-scaling HTTPS endpoint for real-time predictions.  |\n",
    "| **MLOps Pipeline** | **SageMaker Pipelines** | Orchestrates the entire end-to-end workflow: data prep, training, evaluation, model registration, and deployment. |\n",
    "| **Model Registry** | **SageMaker Model Registry** | A central, secure repository to version, catalog, and manage the approval status of trained models before deployment. |\n",
    "| **Feature Management**| **SageMaker Feature Store** | Provides a managed repository for features, ensuring consistency between training and real-time inference. |\n",
    "| **Drift Detection** | **SageMaker Model Monitor** | Automatically monitors the live endpoint for data and model quality drift, triggering alerts when deviations are detected. |\n",
    "| **Bias & Explainability**| **SageMaker Clarify** | Measures for potential bias in training data and post-training, and provides explanations for model predictions. |\n",
    "| **Secret Management** | **AWS Secrets Manager** | Securely stores and manages access to secrets like database credentials and API keys for other services. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **High-Level Architecture Diagram**\n",
    "![SageMaker Deployment Diagram](../resources/images/sagemaker_deployment_architecture_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1.** High-Level Deployment Architecture for the HR Attrition Model.\n",
    "\n",
    "**Inference Path:** Client requests are routed through Amazon API Gateway to a SageMaker Endpoint. The endpoint retrieves real-time data from the SageMaker Feature Store to generate a prediction.\n",
    "\n",
    "**Training Pipeline:** A SageMaker Pipeline automates the MLOps lifecycle, using data from S3 to conduct training jobs. The resulting model artifacts are versioned in the Model Registry before being deployed to the endpoint.\n",
    "\n",
    "**Monitoring:** The SageMaker Endpoint sends logs to Amazon CloudWatch. SageMaker Model Monitor analyzes these logs to detect data and model drift over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. API Endpoint Design\n",
    "\n",
    "The model will be exposed via a managed **SageMaker Real-Time Endpoint**. SageMaker handles the underlying infrastructure, container hosting, and API creation, providing a secure and scalable HTTPS endpoint.\n",
    "\n",
    "* **Endpoint:** A unique HTTPS URL provided by SageMaker.\n",
    "* **Method:** `POST`\n",
    "* **Authentication:** Handled by standard AWS IAM roles and policies, which can be integrated with an Amazon API Gateway for external access control.\n",
    "\n",
    "#### **API Interaction**\n",
    "Clients will interact with the SageMaker Runtime using the AWS SDK or by making an HTTP request to the endpoint URL. The request body is typically a CSV or JSON payload formatted as the model expects.\n",
    "\n",
    "**Example Request Body (JSON):**\n",
    "```json\n",
    "{\n",
    "  \"Age\": 35,\n",
    "  \"JobRole\": \"Sales Executive\",\n",
    "  \"MonthlyIncome\": 5500,\n",
    "  \"YearsAtCompany\": 5,\n",
    "  \"OverTime\": \"Yes\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Monitoring, Maintenance & Retraining\n",
    "\n",
    "Continuous monitoring and automated retraining are managed natively within the AWS SageMaker ecosystem.\n",
    "\n",
    "* **Service Health**: Endpoint metrics like **Latency, Invocations, and Error Rates** are automatically published to **Amazon CloudWatch**, with alarms configured for performance degradation.\n",
    "* **Model & Data Drift**: **SageMaker Model Monitor** is scheduled to run against the live endpoint. It compares the live traffic against the training baseline and triggers CloudWatch Events if drift is detected.\n",
    "* **Fairness & Bias**: **SageMaker Clarify** is used within the pipeline to generate pre-training bias reports and post-training fairness metrics. A new model version will be rejected if fairness metrics (e.g., recall difference across demographics) exceed a predefined threshold.\n",
    "\n",
    "#### **Automated Retraining with SageMaker Pipelines**\n",
    "The entire MLOps workflow is defined and automated as a **SageMaker Pipeline**, which is triggered on a schedule (e.g., monthly) or when significant model drift is detected.\n",
    "\n",
    "1.  **Data Processing**: A processing step pulls the latest data and prepares features using the SageMaker Feature Store.\n",
    "2.  **Train Model**: A training step launches a SageMaker Training Job to train the candidate model.\n",
    "3.  **Evaluate Model**: An evaluation step compares the candidate model's performance (e.g., recall, precision) against a baseline on a hold-out dataset.\n",
    "4.  **Register Model**: If the new model meets the performance criteria, it is versioned and registered in the **SageMaker Model Registry** with a \"Pending Approval\" status.\n",
    "5.  **Deploy**: Upon manual approval (or an automated rule), a final step in the pipeline deploys the approved model version to the production SageMaker Endpoint, automatically updating it with zero downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Risk, Governance & Compliance\n",
    "\n",
    "Managing risk is critical for any HR-related AI system.\n",
    "\n",
    "* **Security**: All traffic is encrypted with TLS. Secrets and credentials are managed natively and securely in **AWS Secrets Manager**. Role-based access control (RBAC) is enforced with IAM policies.\n",
    "* **Privacy & Data Retention**: Input features are logged for audit but purged after 180 days to respect privacy. The system includes a flag to honor employee opt-out requests.\n",
    "* **Ethical Use**: The model serves as a decision-support tool. It is not fully automated. A human manager is always in the loop and must record a reason for any action taken.\n",
    "* **Regulatory Compliance**: The system and its data handling procedures are designed for compliance with GDPR, India's PDP, and EEOC AI guidance, with checks performed quarterly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion and Next Steps\n",
    "\n",
    "The deployment design meets all latency, cost, and governance requirements while leveraging a modern, fully managed MLOps stack on AWS. It provides a clear path to a production-ready system with reduced operational overhead.\n",
    "\n",
    "The next milestone is to implement the **Infrastructure as Code (IaC)** using the **AWS CDK** or **Terraform** to define the SageMaker resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
